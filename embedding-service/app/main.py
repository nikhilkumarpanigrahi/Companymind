"""
FastAPI application entryâ€‘point.

Endpoints:
    GET  /health        â€“ readiness probe
    POST /embed-query   â€“ embed a single search query
    POST /embed-batch   â€“ embed a batch of documents
"""

from __future__ import annotations

import time
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import ORJSONResponse

from app.config import get_settings
from app.embedding import EmbeddingEngine
from app.logger import logger
from app.models import (
    EmbedBatchRequest,
    EmbedBatchResponse,
    EmbedQueryRequest,
    EmbedQueryResponse,
    HealthResponse,
)

settings = get_settings()


# â”€â”€ Lifespan: load model once at startup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Load the embedding model before the server starts accepting traffic."""
    engine = EmbeddingEngine.get_instance()
    engine.load_model()
    logger.info("ðŸš€  %s v%s is ready", settings.APP_NAME, settings.APP_VERSION)
    yield
    logger.info("Shutting down â€¦")


# â”€â”€ App factory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
app = FastAPI(
    title=settings.APP_NAME,
    version=settings.APP_VERSION,
    description=(
        "Productionâ€‘ready embedding microservice for the CompanyMind "
        "Semantic Search Engine.  Generates L2â€‘normalized vector embeddings "
        "using SentenceTransformers."
    ),
    lifespan=lifespan,
    default_response_class=ORJSONResponse,
)

# â”€â”€ CORS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.CORS_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# â”€â”€ Logging middleware â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.middleware("http")
async def log_requests(request: Request, call_next):
    start = time.perf_counter()
    response = await call_next(request)
    elapsed_ms = (time.perf_counter() - start) * 1000
    logger.info(
        "%s %s â†’ %d  (%.1fms)",
        request.method,
        request.url.path,
        response.status_code,
        elapsed_ms,
    )
    return response


# â”€â”€ Health check â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.get(
    "/health",
    response_model=HealthResponse,
    tags=["System"],
    summary="Readiness / health check",
)
async def health():
    engine = EmbeddingEngine.get_instance()
    return HealthResponse(
        status="ok" if engine.is_loaded else "unavailable",
        model_loaded=engine.is_loaded,
        model_name=settings.MODEL_NAME,
        embedding_dimension=engine.dimension,
        version=settings.APP_VERSION,
        cache_stats=engine.cache_stats if engine.is_loaded else None,
    )


# â”€â”€ Single query embedding â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.post(
    "/embed-query",
    response_model=EmbedQueryResponse,
    tags=["Embeddings"],
    summary="Generate embedding for a single query",
)
async def embed_query(payload: EmbedQueryRequest):
    engine = EmbeddingEngine.get_instance()
    if not engine.is_loaded:
        raise HTTPException(status_code=503, detail="Model is not loaded yet.")

    embedding = engine.encode_single(payload.text)
    return EmbedQueryResponse(
        embedding=embedding,
        dimension=engine.dimension,
        model=settings.MODEL_NAME,
    )


# â”€â”€ Batch embedding â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.post(
    "/embed-batch",
    response_model=EmbedBatchResponse,
    tags=["Embeddings"],
    summary="Generate embeddings for a batch of texts",
)
async def embed_batch(payload: EmbedBatchRequest):
    engine = EmbeddingEngine.get_instance()
    if not engine.is_loaded:
        raise HTTPException(status_code=503, detail="Model is not loaded yet.")

    if len(payload.texts) > settings.MAX_BATCH_SIZE:
        raise HTTPException(
            status_code=422,
            detail=(
                f"Batch size {len(payload.texts)} exceeds the maximum "
                f"allowed ({settings.MAX_BATCH_SIZE})."
            ),
        )

    try:
        embeddings = engine.encode_batch(payload.texts)
    except Exception as exc:
        logger.exception("Batch encoding failed")
        raise HTTPException(status_code=500, detail=str(exc)) from exc

    return EmbedBatchResponse(
        embeddings=embeddings,
        dimension=engine.dimension,
        count=len(embeddings),
        model=settings.MODEL_NAME,
    )
